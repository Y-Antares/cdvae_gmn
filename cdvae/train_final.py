#!/usr/bin/env python3
"""
æœ€ç»ˆä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬
è§£å†³æ‰€æœ‰é…ç½®åŠ è½½å’Œæ’å€¼é—®é¢˜
"""

import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pymatgen.io.cif")

import sys
import os
import argparse
import yaml
from pathlib import Path
import re

import numpy as np
import torch
import pytorch_lightning as pl
from pytorch_lightning import seed_everything
from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from omegaconf import DictConfig, OmegaConf
import hydra

# è®¾ç½®ç¯å¢ƒ
PROJECT_ROOT = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ['PROJECT_ROOT'] = str(PROJECT_ROOT)
sys.path.append(str(PROJECT_ROOT))

def parse_args():
    parser = argparse.ArgumentParser(description='Enhanced CDVAE Training')
    parser.add_argument('--data', type=str, default='perov_1k', help='Data config name')
    parser.add_argument('--model', type=str, default='enhanced_cdvae', help='Model config name')
    parser.add_argument('--max_epochs', type=int, default=300, help='Maximum training epochs')
    parser.add_argument('--wandb', action='store_true', help='Enable W&B logging')
    parser.add_argument('--output_dir', type=str, default='results/default', help='Output directory')
    parser.add_argument('--clean_start', action='store_true', help='Start training from scratch')
    return parser.parse_args()

def resolve_interpolations_in_text(text, context):
    """
    æ‰‹åŠ¨è§£ææ–‡æœ¬ä¸­çš„æ’å€¼å˜é‡
    å°† ${key} æˆ– ${path.to.key} æ›¿æ¢ä¸ºå®é™…å€¼
    """
    if not isinstance(text, str):
        return text
    
    # åŒ¹é…æ’å€¼æ¨¡å¼
    pattern = r'\$\{([^}]+)\}'
    
    def replace_interpolation(match):
        var_path = match.group(1)
        
        # ç‰¹æ®Šå¤„ç†ç¯å¢ƒå˜é‡
        if var_path.startswith('oc.env:'):
            env_var = var_path.replace('oc.env:', '')
            if env_var in os.environ:
                return os.environ[env_var]
            else:
                print(f"âš ï¸ ç¯å¢ƒå˜é‡æœªè®¾ç½®: {env_var}")
                return match.group(0)
        
        # è§£æè·¯å¾„
        if '.' in var_path:
            keys = var_path.split('.')
            value = context
            try:
                for key in keys:
                    value = value[key]
                return str(value)
            except (KeyError, TypeError):
                print(f"âš ï¸ æ— æ³•è§£ææ’å€¼å˜é‡: {var_path}")
                return match.group(0)  # è¿”å›åŸå§‹æ–‡æœ¬
        else:
            # ç®€å•å˜é‡å
            if var_path in context:
                return str(context[var_path])
            else:
                print(f"âš ï¸ æ— æ³•è§£ææ’å€¼å˜é‡: {var_path}")
                return match.group(0)  # è¿”å›åŸå§‹æ–‡æœ¬
    
    return re.sub(pattern, replace_interpolation, text)

def resolve_interpolations_in_config(config, context):
    """
    é€’å½’è§£æé…ç½®ä¸­çš„æ‰€æœ‰æ’å€¼å˜é‡
    """
    if isinstance(config, dict):
        resolved = {}
        for key, value in config.items():
            resolved[key] = resolve_interpolations_in_config(value, context)
        return resolved
    elif isinstance(config, list):
        return [resolve_interpolations_in_config(item, context) for item in config]
    elif isinstance(config, str):
        return resolve_interpolations_in_text(config, context)
    else:
        return config

def load_complete_config(data_name, model_name):
    """
    åŠ è½½å®Œæ•´é…ç½®å¹¶è§£å†³æ‰€æœ‰æ’å€¼é—®é¢˜
    """
    print(f"åŠ è½½æ•°æ®é…ç½®: {data_name}")
    data_config_path = PROJECT_ROOT / "conf/data" / f"{data_name}.yaml"
    with open(data_config_path, 'r') as f:
        data_config = yaml.safe_load(f)
    
    print(f"åŠ è½½æ¨¡å‹é…ç½®: {model_name}")
    model_config_path = PROJECT_ROOT / "conf/model" / f"{model_name}.yaml"
    with open(model_config_path, 'r') as f:
        model_config = yaml.safe_load(f)
    
    # åˆ›å»ºæ›´å®Œæ•´çš„æ’å€¼ä¸Šä¸‹æ–‡
    interpolation_context = {
        'data': data_config,
        'model': model_config,
        # æ·»åŠ ç¯å¢ƒå˜é‡
        'oc': {
            'env': {
                'PROJECT_ROOT': str(PROJECT_ROOT)
            }
        },
        # æ·»åŠ  Hydra ç›¸å…³å˜é‡
        'expname': f"{data_name}_{model_name}",
        'now': {
            '%Y-%m-%d': '2024-06-10',
            '%H-%M-%S': '12-00-00'
        },
        'id': '001',
        'num': '1',
        # å°†dataå’Œmodelä¸­çš„æ‰€æœ‰å˜é‡æå‡åˆ°é¡¶å±‚
        **data_config,
        # æ·»åŠ ä¸€äº›modelçš„å…³é”®å˜é‡åˆ°é¡¶å±‚
        'latent_dim': model_config.get('latent_dim', 256),
        'max_neighbors': model_config.get('max_neighbors', 20),
        'radius': model_config.get('radius', 7.0),
    }
    
    print("è§£ææ¨¡å‹é…ç½®ä¸­çš„ defaults...")
    
    # å¤„ç† defaults ä¸­çš„ encoder å’Œ decoder
    if 'defaults' in model_config:
        for default_item in model_config['defaults']:
            if isinstance(default_item, dict):
                if 'encoder' in default_item:
                    encoder_name = default_item['encoder']
                    encoder_path = PROJECT_ROOT / "conf/model/encoder" / f"{encoder_name}.yaml"
                    print(f"  åŠ è½½ç¼–ç å™¨: {encoder_name}")
                    
                    with open(encoder_path, 'r') as f:
                        encoder_config = yaml.safe_load(f)
                    
                    # è§£æç¼–ç å™¨é…ç½®ä¸­çš„æ’å€¼
                    print("  è§£æç¼–ç å™¨æ’å€¼...")
                    encoder_config = resolve_interpolations_in_config(encoder_config, interpolation_context)
                    model_config['encoder'] = encoder_config
                
                if 'decoder' in default_item:
                    decoder_name = default_item['decoder']
                    decoder_path = PROJECT_ROOT / "conf/model/decoder" / f"{decoder_name}.yaml"
                    print(f"  åŠ è½½è§£ç å™¨: {decoder_name}")
                    
                    with open(decoder_path, 'r') as f:
                        decoder_config = yaml.safe_load(f)
                    
                    # è§£æè§£ç å™¨é…ç½®ä¸­çš„æ’å€¼
                    print("  è§£æè§£ç å™¨æ’å€¼...")
                    decoder_config = resolve_interpolations_in_config(decoder_config, interpolation_context)
                    model_config['decoder'] = decoder_config
    
    # è§£ææ¨¡å‹é…ç½®ä¸­å‰©ä½™çš„æ’å€¼
    print("è§£ææ¨¡å‹é…ç½®ä¸­çš„å…¶ä»–æ’å€¼...")
    model_config = resolve_interpolations_in_config(model_config, interpolation_context)
    
    # è§£ææ•°æ®é…ç½®ä¸­çš„æ’å€¼
    print("è§£ææ•°æ®é…ç½®ä¸­çš„æ’å€¼...")
    data_config = resolve_interpolations_in_config(data_config, interpolation_context)
    
    # ç§»é™¤ defaults é”®ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨å¤„ç†äº†
    if 'defaults' in model_config:
        del model_config['defaults']
    
    # åŠ è½½å…¶ä»–é»˜è®¤é…ç½®
    default_configs = {}
    
    # åŠ è½½è®­ç»ƒé…ç½®
    train_config_path = PROJECT_ROOT / "conf/train/default.yaml"
    if train_config_path.exists():
        with open(train_config_path, 'r') as f:
            train_config = yaml.safe_load(f)
        default_configs['train'] = train_config
    
    # åŠ è½½ä¼˜åŒ–å™¨é…ç½®
    optim_config_path = PROJECT_ROOT / "conf/optim/default.yaml"
    if optim_config_path.exists():
        with open(optim_config_path, 'r') as f:
            optim_config = yaml.safe_load(f)
        default_configs['optim'] = optim_config
    
    # åŠ è½½æ—¥å¿—é…ç½®
    logging_config_path = PROJECT_ROOT / "conf/logging/default.yaml"
    if logging_config_path.exists():
        with open(logging_config_path, 'r') as f:
            logging_config = yaml.safe_load(f)
        default_configs['logging'] = logging_config
    
    print("é…ç½®åŠ è½½å®Œæˆï¼Œç»„è£…æœ€ç»ˆé…ç½®...")
    
    # ç»„è£…æœ€ç»ˆé…ç½®
    final_config = {
        'data': data_config,
        'model': model_config,
        **default_configs
    }
    
    return OmegaConf.create(final_config)

def fix_numeric_types(config):
    """ä¿®å¤é…ç½®ä¸­çš„æ•°å€¼ç±»å‹"""
    if isinstance(config, dict):
        for key, value in config.items():
            if isinstance(value, str):
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                try:
                    if '.' in value:
                        config[key] = float(value)
                    else:
                        config[key] = int(value)
                except ValueError:
                    pass  # ä¿æŒå­—ç¬¦ä¸²
            elif isinstance(value, (dict, list)):
                fix_numeric_types(value)
    elif isinstance(config, list):
        for item in config:
            fix_numeric_types(item)

def fix_all_numeric_types(cfg):
    """ä¿®å¤æ•´ä¸ªé…ç½®ä¸­çš„æ•°å€¼ç±»å‹"""
    print("ä¿®å¤æ•°å€¼ç±»å‹...")
    
    # ä¿®å¤ä¼˜åŒ–å™¨é…ç½®
    if hasattr(cfg, 'optim'):
        fix_numeric_types(cfg.optim)
    
    # ä¿®å¤æ•°æ®é…ç½®
    if hasattr(cfg, 'data'):
        fix_numeric_types(cfg.data)
        # ç‰¹åˆ«æ³¨æ„è¿™äº›å…³é”®çš„æ•°å€¼å‚æ•°
        numeric_keys = [
            'preprocess_workers', 'num_targets', 'max_atoms', 
            'train_max_epochs', 'early_stopping_patience', 
            'teacher_forcing_max_epoch'
        ]
        for key in numeric_keys:
            if hasattr(cfg.data, key) and isinstance(getattr(cfg.data, key), str):
                try:
                    setattr(cfg.data, key, int(getattr(cfg.data, key)))
                    print(f"  è½¬æ¢ {key}: {getattr(cfg.data, key)} (int)")
                except ValueError:
                    pass
    
    # ä¿®å¤æ¨¡å‹é…ç½®
    if hasattr(cfg, 'model'):
        fix_numeric_types(cfg.model)
    
    # ä¿®å¤è®­ç»ƒé…ç½®
    if hasattr(cfg, 'train'):
        fix_numeric_types(cfg.train)

def patch_minmax_scaler():
    """ä¿®è¡¥ MinMaxScalerTorch çš„ copy æ–¹æ³•"""
    from cdvae.common.data_utils import MinMaxScalerTorch
    
    original_copy = MinMaxScalerTorch.copy
    
    def fixed_copy(self):
        new_scaler = MinMaxScalerTorch(
            min_val=self.min_val,
            max_val=self.max_val,
            mins=self.mins.clone().detach() if self.mins is not None else None,
            maxs=self.maxs.clone().detach() if self.maxs is not None else None
        )
        # ç¡®ä¿å¤åˆ¶ ranges å±æ€§
        if hasattr(self, 'ranges') and self.ranges is not None:
            new_scaler.ranges = self.ranges.clone().detach()
        return new_scaler
    
    MinMaxScalerTorch.copy = fixed_copy
    print("âœ… å·²ä¿®è¡¥ MinMaxScalerTorch.copy æ–¹æ³•")

def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Enhanced CDVAE Training")
    print(f"Data: {args.data}")
    print(f"Model: {args.model}")
    print(f"Output directory: {output_dir}")
    print(f"Max epochs: {args.max_epochs}")
    
    # å¤„ç†æ£€æŸ¥ç‚¹
    if args.clean_start:
        ckpts = list(output_dir.glob('*.ckpt'))
        if ckpts:
            backup_dir = output_dir / "backup_checkpoints"
            backup_dir.mkdir(exist_ok=True)
            for ckpt in ckpts:
                ckpt.rename(backup_dir / ckpt.name)
            print(f"Moved {len(ckpts)} checkpoints to backup")
    
    # åº”ç”¨è¡¥ä¸
    patch_minmax_scaler()
    
    try:
        # åŠ è½½å®Œæ•´é…ç½®
        cfg = load_complete_config(args.data, args.model)
        
        # ä¿®å¤æ‰€æœ‰æ•°å€¼ç±»å‹
        fix_all_numeric_types(cfg)
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        cfg.train.pl_trainer.max_epochs = args.max_epochs
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹é…ç½®é”®: {list(cfg.model.keys())}")
        print(f"åŒ…å« encoder: {'encoder' in cfg.model}")
        print(f"åŒ…å« decoder: {'decoder' in cfg.model}")
        
        # è®¾ç½®éšæœºç§å­
        seed_everything(42)
        
        # å®ä¾‹åŒ–æ•°æ®æ¨¡å—ä¹‹å‰ï¼Œç¡®ä¿æ•°æ®é›†é…ç½®ä¸­çš„æ•°å€¼ç±»å‹æ­£ç¡®
        print("ä¿®å¤æ•°æ®é›†é…ç½®ä¸­çš„æ•°å€¼ç±»å‹...")
        if hasattr(cfg.data.datamodule, 'datasets'):
            for dataset_name in ['train', 'val', 'test']:
                if hasattr(cfg.data.datamodule.datasets, dataset_name):
                    dataset_cfg = getattr(cfg.data.datamodule.datasets, dataset_name)
                    if isinstance(dataset_cfg, list):
                        # val å’Œ test å¯èƒ½æ˜¯åˆ—è¡¨
                        for i, ds_cfg in enumerate(dataset_cfg):
                            fix_numeric_types(ds_cfg)
                            # ç‰¹åˆ«å¤„ç† preprocess_workers
                            if hasattr(ds_cfg, 'preprocess_workers') and isinstance(ds_cfg.preprocess_workers, str):
                                ds_cfg.preprocess_workers = int(ds_cfg.preprocess_workers)
                                print(f"  ä¿®å¤ {dataset_name}[{i}].preprocess_workers: {ds_cfg.preprocess_workers}")
                    else:
                        # train é€šå¸¸æ˜¯å•ä¸ªé…ç½®
                        fix_numeric_types(dataset_cfg)
                        # ç‰¹åˆ«å¤„ç† preprocess_workers
                        if hasattr(dataset_cfg, 'preprocess_workers') and isinstance(dataset_cfg.preprocess_workers, str):
                            dataset_cfg.preprocess_workers = int(dataset_cfg.preprocess_workers)
                            print(f"  ä¿®å¤ {dataset_name}.preprocess_workers: {dataset_cfg.preprocess_workers}")
        
        # å®ä¾‹åŒ–æ•°æ®æ¨¡å—
        print(f"Instantiating <{cfg.data.datamodule._target_}>")
        datamodule = hydra.utils.instantiate(cfg.data.datamodule, _recursive_=False)
        
        # å®ä¾‹åŒ–æ¨¡å‹
        print(f"Instantiating <{cfg.model._target_}>")
        model = hydra.utils.instantiate(
            cfg.model,
            optim=cfg.optim,
            data=cfg.data,
            logging=cfg.logging,
            _recursive_=False,
        )
        
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"æ¨¡å‹ç±»å‹: {type(model)}")
        
        # ä¼ é€’æ ‡å‡†åŒ–å™¨
        print("Setting up scalers...")
        model.lattice_scaler = datamodule.lattice_scaler.copy()
        model.scaler = datamodule.scaler.copy()
        
        # ä¼ é€’èƒ½é‡æ ‡å‡†åŒ–å™¨
        if hasattr(datamodule, 'energy_scaler') and datamodule.energy_scaler is not None:
            model.energy_scaler = datamodule.energy_scaler.copy()
            torch.save(datamodule.energy_scaler, output_dir / 'energy_scaler.pt')
        else:
            from cdvae.common.data_utils import StandardScaler
            model.energy_scaler = StandardScaler(mean=0.0, std=1.0)
            torch.save(model.energy_scaler, output_dir / 'energy_scaler.pt')
        
        # ä¿å­˜å…¶ä»–æ ‡å‡†åŒ–å™¨
        torch.save(datamodule.lattice_scaler, output_dir / 'lattice_scaler.pt')
        torch.save(datamodule.scaler, output_dir / 'prop_scaler.pt')
        
        # æ„å»ºå›è°ƒ
        callbacks = []
        
        callbacks.append(LearningRateMonitor(
            logging_interval='step',
            log_momentum=True,
        ))
        
        callbacks.append(EarlyStopping(
            monitor='val_loss',
            mode='min',
            patience=50,
            verbose=True,
        ))
        
        callbacks.append(ModelCheckpoint(
            dirpath=output_dir,
            monitor='val_loss',
            mode='min',
            save_top_k=3,
            verbose=True,
            filename='epoch={epoch:02d}-val_loss={val_loss:.3f}',
        ))
        
        # è®¾ç½®logger
        wandb_logger = None
        if args.wandb:
            wandb_logger = WandbLogger(
                project='enhanced-cdvae',
                name=f"{args.data}_{args.model}",
                save_dir=output_dir,
                tags=['enhanced', 'gradnorm', 'multi-objective'],
            )
            wandb_logger.watch(model, log='gradients', log_freq=100)
        
        # ä¿å­˜é…ç½®
        yaml_conf = OmegaConf.to_yaml(cfg=cfg)
        (output_dir / "hparams.yaml").write_text(yaml_conf)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹
        ckpts = list(output_dir.glob('*.ckpt'))
        ckpt = None
        if ckpts and not args.clean_start:
            ckpt_epochs = np.array([int(ckpt.parts[-1].split('-')[0].split('=')[1]) for ckpt in ckpts])
            ckpt = str(ckpts[ckpt_epochs.argsort()[-1]])
            print(f"Found checkpoint: {ckpt}")
        
        # å®ä¾‹åŒ–è®­ç»ƒå™¨
        print("Instantiating the Trainer")
        trainer = pl.Trainer(
            default_root_dir=output_dir,
            logger=wandb_logger,
            callbacks=callbacks,
            deterministic=True,
            max_epochs=args.max_epochs,
            check_val_every_n_epoch=1,
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            enable_progress_bar=True,
            enable_model_summary=True,
        )
        
        # è®°å½•è¶…å‚æ•°
        from cdvae.common.utils import log_hyperparameters
        log_hyperparameters(trainer=trainer, model=model, cfg=cfg)
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ Starting training!")
        trainer.fit(model=model, datamodule=datamodule, ckpt_path=ckpt)
        
        print("ğŸ§ª Starting testing!")
        trainer.test(datamodule=datamodule)
        
        print(f"ğŸ‰ Training completed! Results saved to: {output_dir}")
        
        # å…³é—­logger
        if wandb_logger is not None:
            wandb_logger.experiment.finish()
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)