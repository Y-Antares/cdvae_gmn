#!/usr/bin/env python3
"""
åŸºäºåŸå§‹ Hydra çš„å¢å¼ºè®­ç»ƒè„šæœ¬
ä½¿ç”¨ Hydra çš„ compose API é¿å…æ’å€¼é—®é¢˜
"""

import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pymatgen.io.cif")

import sys
import os
import argparse
from pathlib import Path

import numpy as np
import torch
import pytorch_lightning as pl
from pytorch_lightning import seed_everything
from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from omegaconf import DictConfig, OmegaConf
import hydra
from hydra import compose, initialize_config_dir

# è®¾ç½®ç¯å¢ƒ
PROJECT_ROOT = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ['PROJECT_ROOT'] = str(PROJECT_ROOT)
sys.path.append(str(PROJECT_ROOT))

def parse_args():
    parser = argparse.ArgumentParser(description='Enhanced CDVAE Training')
    parser.add_argument('--data', type=str, default='perov_1k', help='Data config name')
    parser.add_argument('--model', type=str, default='enhanced_cdvae', help='Model config name')
    parser.add_argument('--max_epochs', type=int, default=300, help='Maximum training epochs')
    parser.add_argument('--wandb', action='store_true', help='Enable W&B logging')
    parser.add_argument('--output_dir', type=str, default='results/default', help='Output directory')
    parser.add_argument('--clean_start', action='store_true', help='Start training from scratch')
    return parser.parse_args()

def patch_minmax_scaler():
    """ä¿®è¡¥ MinMaxScalerTorch çš„ copy æ–¹æ³•"""
    from cdvae.common.data_utils import MinMaxScalerTorch
    
    original_copy = MinMaxScalerTorch.copy
    
    def fixed_copy(self):
        new_scaler = MinMaxScalerTorch(
            min_val=self.min_val,
            max_val=self.max_val,
            mins=self.mins.clone().detach() if self.mins is not None else None,
            maxs=self.maxs.clone().detach() if self.maxs is not None else None
        )
        # ç¡®ä¿å¤åˆ¶ ranges å±æ€§
        if hasattr(self, 'ranges') and self.ranges is not None:
            new_scaler.ranges = self.ranges.clone().detach()
        return new_scaler
    
    MinMaxScalerTorch.copy = fixed_copy
    print("âœ… å·²ä¿®è¡¥ MinMaxScalerTorch.copy æ–¹æ³•")

def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡
    os.environ.setdefault('HYDRA_JOBS', str(output_dir.parent / 'hydra_jobs'))
    os.environ.setdefault('WANDB_DIR', str(output_dir.parent / 'wandb'))
    
    print(f"Enhanced CDVAE Training (Hydra Mode)")
    print(f"Data: {args.data}")
    print(f"Model: {args.model}")
    print(f"Output directory: {output_dir}")
    print(f"Max epochs: {args.max_epochs}")
    
    # å¤„ç†æ£€æŸ¥ç‚¹
    if args.clean_start:
        ckpts = list(output_dir.glob('*.ckpt'))
        if ckpts:
            backup_dir = output_dir / "backup_checkpoints"
            backup_dir.mkdir(exist_ok=True)
            for ckpt in ckpts:
                ckpt.rename(backup_dir / ckpt.name)
            print(f"Moved {len(ckpts)} checkpoints to backup")
    
    # åº”ç”¨è¡¥ä¸
    patch_minmax_scaler()
    
    try:
        # ä½¿ç”¨ Hydra compose API
        config_dir = str(PROJECT_ROOT / "conf")
        
        with initialize_config_dir(config_dir=config_dir, version_base="1.3"):
            # ç»„åˆé…ç½®ï¼Œä½¿ç”¨å®Œæ•´çš„ Hydra ç³»ç»Ÿ
            cfg = compose(
                config_name="default",
                overrides=[
                    f"data={args.data}",
                    f"model={args.model}",
                    f"train.pl_trainer.max_epochs={args.max_epochs}",
                    f"expname={args.data}_{args.model}",
                    f"hydra.run.dir={output_dir}",  # è¦†ç›–è¾“å‡ºç›®å½•
                    "hydra.job.chdir=False",  # ä¸æ”¹å˜å·¥ä½œç›®å½•
                ]
            )
            
            print("âœ… Hydra é…ç½®åˆ›å»ºæˆåŠŸ")
            print(f"æ¨¡å‹é…ç½®é”®: {list(cfg.model.keys())}")
            print(f"åŒ…å« encoder: {'encoder' in cfg.model}")
            print(f"åŒ…å« decoder: {'decoder' in cfg.model}")
            
            # è®¾ç½®éšæœºç§å­
            if cfg.train.deterministic:
                seed_everything(cfg.train.random_seed)
            
            # å®ä¾‹åŒ–æ•°æ®æ¨¡å—
            print(f"Instantiating <{cfg.data.datamodule._target_}>")
            datamodule = hydra.utils.instantiate(cfg.data.datamodule, _recursive_=False)
            
            # å®ä¾‹åŒ–æ¨¡å‹
            print(f"Instantiating <{cfg.model._target_}>")
            model = hydra.utils.instantiate(
                cfg.model,
                optim=cfg.optim,
                data=cfg.data,
                logging=cfg.logging,
                _recursive_=False,
            )
            
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"æ¨¡å‹ç±»å‹: {type(model)}")
            
            # ä¼ é€’æ ‡å‡†åŒ–å™¨
            print("Setting up scalers...")
            model.lattice_scaler = datamodule.lattice_scaler.copy()
            model.scaler = datamodule.scaler.copy()
            
            # ä¼ é€’èƒ½é‡æ ‡å‡†åŒ–å™¨
            if hasattr(datamodule, 'energy_scaler') and datamodule.energy_scaler is not None:
                model.energy_scaler = datamodule.energy_scaler.copy()
                torch.save(datamodule.energy_scaler, output_dir / 'energy_scaler.pt')
            else:
                from cdvae.common.data_utils import StandardScaler
                model.energy_scaler = StandardScaler(mean=0.0, std=1.0)
                torch.save(model.energy_scaler, output_dir / 'energy_scaler.pt')
            
            # ä¿å­˜å…¶ä»–æ ‡å‡†åŒ–å™¨
            torch.save(datamodule.lattice_scaler, output_dir / 'lattice_scaler.pt')
            torch.save(datamodule.scaler, output_dir / 'prop_scaler.pt')
            
            # æ„å»ºå›è°ƒ
            callbacks = []
            
            if "lr_monitor" in cfg.logging:
                callbacks.append(LearningRateMonitor(
                    logging_interval=cfg.logging.lr_monitor.logging_interval,
                    log_momentum=cfg.logging.lr_monitor.log_momentum,
                ))
            
            if "early_stopping" in cfg.train:
                callbacks.append(EarlyStopping(
                    monitor=cfg.train.monitor_metric,
                    mode=cfg.train.monitor_metric_mode,
                    patience=cfg.train.early_stopping.patience,
                    verbose=cfg.train.early_stopping.verbose,
                ))
            
            if "model_checkpoints" in cfg.train:
                callbacks.append(ModelCheckpoint(
                    dirpath=output_dir,
                    monitor=cfg.train.monitor_metric,
                    mode=cfg.train.monitor_metric_mode,
                    save_top_k=cfg.train.model_checkpoints.save_top_k,
                    verbose=cfg.train.model_checkpoints.verbose,
                    filename='epoch={epoch:02d}-val_loss={val_loss:.3f}',
                ))
            
            # æ‰‹åŠ¨æ·»åŠ åŸºç¡€å›è°ƒï¼ˆå¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼‰
            if not any(isinstance(cb, LearningRateMonitor) for cb in callbacks):
                callbacks.append(LearningRateMonitor(logging_interval='step', log_momentum=True))
            
            if not any(isinstance(cb, EarlyStopping) for cb in callbacks):
                callbacks.append(EarlyStopping(monitor='val_loss', mode='min', patience=50, verbose=True))
            
            if not any(isinstance(cb, ModelCheckpoint) for cb in callbacks):
                callbacks.append(ModelCheckpoint(
                    dirpath=output_dir,
                    monitor='val_loss',
                    mode='min',
                    save_top_k=3,
                    verbose=True,
                    filename='epoch={epoch:02d}-val_loss={val_loss:.3f}',
                ))
            
            # è®¾ç½®logger
            wandb_logger = None
            if args.wandb:
                wandb_logger = WandbLogger(
                    project='enhanced-cdvae',
                    name=f"{args.data}_{args.model}",
                    save_dir=output_dir,
                    tags=['enhanced', 'gradnorm', 'multi-objective'],
                )
                wandb_logger.watch(model, log='gradients', log_freq=100)
            
            # ä¿å­˜é…ç½®
            yaml_conf = OmegaConf.to_yaml(cfg=cfg)
            (output_dir / "hparams.yaml").write_text(yaml_conf)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹
            ckpts = list(output_dir.glob('*.ckpt'))
            ckpt = None
            if ckpts and not args.clean_start:
                ckpt_epochs = np.array([int(ckpt.parts[-1].split('-')[0].split('=')[1]) for ckpt in ckpts])
                ckpt = str(ckpts[ckpt_epochs.argsort()[-1]])
                print(f"Found checkpoint: {ckpt}")
            
            # å®ä¾‹åŒ–è®­ç»ƒå™¨
            print("Instantiating the Trainer")
            trainer = pl.Trainer(
                default_root_dir=output_dir,
                logger=wandb_logger,
                callbacks=callbacks,
                deterministic=cfg.train.deterministic,
                max_epochs=args.max_epochs,
                check_val_every_n_epoch=cfg.logging.val_check_interval,
                accelerator='gpu' if torch.cuda.is_available() else 'cpu',
                devices=1,
                enable_progress_bar=True,
                enable_model_summary=True,
            )
            
            # è®°å½•è¶…å‚æ•°
            from cdvae.common.utils import log_hyperparameters
            log_hyperparameters(trainer=trainer, model=model, cfg=cfg)
            
            # å¼€å§‹è®­ç»ƒ
            print("ğŸš€ Starting training!")
            trainer.fit(model=model, datamodule=datamodule, ckpt_path=ckpt)
            
            print("ğŸ§ª Starting testing!")
            trainer.test(datamodule=datamodule)
            
            print(f"ğŸ‰ Training completed! Results saved to: {output_dir}")
            
            # å…³é—­logger
            if wandb_logger is not None:
                wandb_logger.experiment.finish()
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)